---
title: "Feature Selection in LM / subsetting features"
author: "GWU Intro to Data Science DATS 6101"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: true
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r base_lib}
loadPkg("ggplot2")
```


# Feature Selection (step-wise)

## `leaps::regsubsets()`  
Let us use the baseball player dataset (in library `ISLR`) to try feature selection in LM.  
```{r}
loadPkg("ISLR")
str(Hitters)
summary(Hitters)
```

One thing we can do right away is to clean up the data a little. Remove variables that we know for sure are useless.
In our case, let us remove some cumulative player stats to make the dataframe more manageable.

```{r}
hittersC = Hitters[,c('AtBat','Hits','HmRun','Runs','RBI','Walks','Years','League','Division','PutOuts','Assists','Errors','Salary')] # cleaned dataset
unloadPkg("ISLR")
```

```{r}
# loadPkg(pastecs)
# stat.desc(hittersC)
# unloadPkg(pastecs)
```

### Exhaustive search  

```{r}
loadPkg("leaps")
#This is essentially best fit 
reg.best10 <- regsubsets(Salary~. , data = hittersC, nvmax = 10, nbest = 1, method = "exhaustive")  # leaps::regsubsets() - Model selection by exhaustive (default) search, forward or backward stepwise, or sequential replacement
#The plot will show the Adjust R^2 when using the variables across the bottom
plot(reg.best10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.best10, scale = "r2", main = "R^2")
# In the "leaps" package, we can use scale=c("bic","Cp","adjr2","r2")
plot(reg.best10, scale = "bic", main = "BIC")
plot(reg.best10, scale = "Cp", main = "Cp")
summary(reg.best10)
```

We used `nvmax=10` and `nbest=2` as options here. As a result, you will find two 10-variable models, two 9-variable models, two 8-variable models, etc. Can you find them in the graphical output?

In general, BIC criteria tends to favor more predictive model, with fewer regressors, while AIC favors more explanatory model with more regressors.

Also notice how the categorical variables are being treated. Right now, such two variables each only has TWO values/levels, so we only need one coefficient for each. In general, there will be a whole lot more variables needed for more levels.

Let us try non-linear model as well using feature selection with exhaustive method (`nbest=1`)

```{r}
regnonlin.forward10 <- regsubsets(Salary~(AtBat+Hits+Walks+Runs+RBI)^2+League:Division, data = hittersC, nvmax = 10)  # leaps, regsubsets: Model selection by exhaustive search, forward or backward stepwise, or sequential replacement
#The plot will show the Adjust R^2 when using the variables across the bottom
plot(regnonlin.forward10, scale = "adjr2", main = "Adjusted R^2")
plot(regnonlin.forward10, scale = "bic", main = "BIC")
plot(regnonlin.forward10, scale = "Cp", main = "Cp")
summary(regnonlin.forward10)
```

We can also use forward, backward, and seqrep (sequential replacement) methods to see if our results are any different.

From <http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/>
1. Forward selection, which starts with no predictors in the model (the null model), iteratively adds the most contributive predictors, and stops when the improvement is no longer statistically significant.
2. Backward selection (or backward elimination), which starts with all predictors in the model (full model), iteratively removes the least contributive predictors, and stops when you have a model where all predictors are statistically significant.
3. Sequential replacement is sort of a combination of forward and backward selections. You start with no predictors, then sequentially add the most contributive predictors (like forward selection). After adding each new variable, remove any variables that no longer provide an improvement in the model fit (like backward selection).

### Forward Selection

The result for forward selection (with `nvmax=10` and `nbest=2`) is here. We typically do not use the regular r2 as criteria, which usually improves with number of variables and leads to overfitting.  

```{r}
reg.forward10 <- regsubsets(Salary~., data = hittersC, nvmax = 10, nbest = 1, method = "forward")
plot(reg.forward10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.forward10, scale = "bic", main = "BIC")
plot(reg.forward10, scale = "Cp", main = "Cp")
# summary(reg.forward10)
```

Can we trace the steps and procedures of the selection one level at a time? Start with the 1-var model, then 2-var, then 3?

And the non-linear model:

```{r}
regnonlin.forward10 <- regsubsets(Salary~(AtBat+Hits+Walks+Runs+RBI)^2+League:Division, data = hittersC, nvmax = 10, method="forward")  # leaps, regsubsets: Model selection by exhaustive search, forward or backward stepwise, or sequential replacement
plot(regnonlin.forward10, scale = "adjr2", main = "Adjusted R^2")
plot(regnonlin.forward10, scale = "bic", main = "BIC")
plot(regnonlin.forward10, scale = "Cp", main = "Cp")
summary(regnonlin.forward10)
```


### Backward Selection

Now backwards (`nvmax=10` and `nbest=2`)

```{r}
reg.back10 <- regsubsets(Salary~., data = hittersC, method = "backward", nvmax = 10, nbest = 2)
plot(reg.back10, scale = "adjr2", main = "Adjusted R^2")
plot(reg.back10, scale = "bic", main = "BIC")
plot(reg.back10, scale = "Cp", main = "Cp")
summary(reg.back10)
```

Again, can we trace the steps and procedures one level at a time? Start with the 10-var model, then 9-var, then 3?  


And the non-linear model (`nbest=1`).

```{r}
regnonlin.back <- regsubsets(Salary~(AtBat+Hits+Walks+Runs+RBI)^2+League:Division, data = hittersC, method = "backward", nvmax = 10)
plot(regnonlin.back, scale = "adjr2", main = "Adjusted R^2")
plot(regnonlin.back, scale = "bic", main = "BIC")
plot(regnonlin.back, scale = "Cp", main = "Cp")
summary(regnonlin.back)
```

### Sequential Replacement seqrep  

Lastly we can try seqrep method.
```{r}
reg.seqrep <- regsubsets(Salary~., data = hittersC, nvmax = 10, nbest = 2 , method = "seqrep")
plot(reg.seqrep, scale = "adjr2", main = "Adjusted R^2")
plot(reg.seqrep, scale = "bic", main = "BIC")
plot(reg.seqrep, scale = "Cp", main = "Cp")
```



## `car::subsets()`

Next, let us look at the results again using a differnt presentation. Instead of plotting the reg.forward10 results from the generic `graphics::plot()` function, we will use the `car` library function `car::subsets()` to make a different plot.
The treatment is modified from an [online source here](https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html).  

### Basic plots
We need the object resulting from `regsubsets()` for more advanced plotting. So let us first re-create a result for the default plots. 

```{r}

reg2.best <- regsubsets(Salary~. , data = hittersC, nvmax = 9, nbest = 1, method = "exhaustive") 
plot(reg2.best, scale = "adjr2", main = "Adjusted R^2")
plot(reg2.best, scale = "r2", main = "R^2")
plot(reg2.best, scale = "bic", main = "BIC")
plot(reg2.best, scale = "Cp", main = "Cp")
summary(reg2.best)
```
### Alternative plots  
We can then use the function `car::subsets()` to get some more advanced/alternative plotting. Below shows that the 6-var model AB-Ht-W-Y-D-P is at or close to the highest Adjusted $R^2$.  

```{r}
loadPkg("car")
loadPkg("faraway")

summaryRegForward = summary(reg2.best)
# Adjusted R2
car::subsets(reg2.best, statistic="adjr2", legend = FALSE, min.size = 2, main = "Adjusted R^2")
```

If we use Mallow Cp to decide on the regressors, the "stopping rule is to start with the smallest model and gradually increase number of variables, and stop when Mallow Cp is approximately (number of regressors + 1, broken line) for the first time." (from [this article](https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html)). In our case, it happened close to broken line at the 5-var model AB-Ht-W-Y-P, and the 6-var model with AB-Ht-W-Y-D-P.  

```{r}
# 
# Mallow Cp
subsets(reg2.best, statistic="cp", legend = FALSE, min.size = 5, main = "Mallow Cp")
abline(a = 1, b = 1, lty = 3)  
# a: intercept; b: slope, 
# lty: line-type (0=blank, 1=solid (default), 2=dashed, 3=dotted, 4=dotdash, 5=longdash, 6=twodash) 
#
# this output gives the list of variables and their abbreviations
```

You can now try to go back and change `nbest = 2` on (or about) line 186, and re-run the codes.  You should see more alternative results.




