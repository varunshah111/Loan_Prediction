# Team_2_America
Midterm Project

Team 2 Project Proposal
Cooper Atkins, Ricardo Diaz, Varun Shah

Our team will be studying loans in India and their default rates based on customer characteristics. Our primary research questions are as follows:

•	Do customers who default on loans have statistically lower incomes than those who don't default?

•	Does homeownership correlate with lower rates of default?

•	Does being married decrease the likelihood of default?

•	Does an additional year of homeownership reduce the likelihood of default?

•	Does job experience or age show a larger impact on someone defaulting on their loan?

We recognize that not all of these questions can be answered with EDA including Z-Tests/T-Tests and Chi- Squares, but are choosing to pose these questions now for consideration when we eventually develop a model based on this dataset.

Our dataset is from Kaggle and can be found here and we will be using the "Training Data" dataset, which has around 252K data points.


Our team will continue studying loans in India and their default rates based on customer characteristics. Our primary research questions to be answered with a logistic regression model are as follows: 
 

Despite not being useful on it's own, is income statistically significant in a model when other variables are included? 

Does current job experience or overall job experience yield a better model? 

For each additional year, how does age impact likelihood of defaulting on a loan? 

For each additional year, how does job experience (whichever version was deemed more significant before) impact likelihood of defaulting on a loan? 

Does manual, exhaustive stepwise, or other modelling techniques produce a better model using the different methods (AIC, BIC, pseudo-R^2 etc.) 

Does each method for model selection produce a significant model as determined by ROC-AUC >= 0.8, and if so, which produces the best? Does it agree with what we said before? 

What are the most significant predictors for default, and do they appear across all of the "best" models? 

Using a confusion matrix, which of our "best" models appears to perform best across the different metrics we care about (precision, recall-rate, etc.) 

We also have a test dataset (we used our training dataset to build the model). How do the different models fare when used on the test dataset? 


We are hoping to answer the last question, but we may run into time limitations, so we are prioritizing our model selection. 

 

Our dataset is from Kaggle and can be found here and we will be using the "Training Data" dataset, which has around 252K data points. 

 